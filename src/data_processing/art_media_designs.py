
# # Create media designs suggested by ART


# We use ART to provide suggested designs for media components for which to get phenotypic data. 
# 
# For DBTL 4 we create 8 designs from an exploratory mode with $\alpha=1.$, 7 designs from the exploitation mode ($\alpha=0$) and 1 design being close to the standard media as a control, totalling 16 designs in triplicates. 


# Tested using **ART 3.9.4** kernel on jprime.lbl.gov


# ## Inputs and output


# **Required files to run this notebook:**
# - `Putida_media_bounds.csv`
# - `standard_recipe_concentrations.csv`
# - EDD study slug(s)


# **File generated by running this notebook**
# 
# - 


# ## Setup


# Clone the git repository with the `ART` library 
# 
# `git clone https://github.com/JBEI/AutomatedRecommendationTool.git`  
# <!-- <font color='red'> _____ -->
# <!-- **WE SHOULD TALK ABOUT LICENSING HERE!!!** </font> -->
# 
# or pull the latest version. 
# 
# Information about licensing ART is available at https://github.com/JBEI/ART.
# 
# Importing needed libraries:


import sys
sys.path.append('../../AutomatedRecommendationTool')        # Make sure this is the location for the ART library 
sys.path.append('../')
import re
import matplotlib.pyplot as plt
import warnings
    
from art.core import *
import art.plot as plot
import edd_utils as eddu

from core import designs_pairwise


# ## User parameters


CYCLE = 4

user_params = {
    'bounds_file': f'../data/flaviolin/Putida_media_bounds.csv',
    'output_file_path': f'../data/flaviolin/DBTL{CYCLE}', # Folder for output files,
    'standard_media_file': '../data/flaviolin/standard_recipe_concentrations.csv',
    'study_slug_1': 'combinatorial-media-for-flaviolin-dbtl1-ed1e',
    'study_slug_2': 'combinatorial-media-for-flaviolin-dbtl-2',
    'study_slug_3': 'combinatorial-media-for-flaviolin-dbtl-3',
    'edd_server': 'edd.jbei.org',
    'username': 'tradivojevic',
}



# Specify which components to explore and which response to optimize:


user_params['components'] = [
    'H3BO3',
    'K2SO4',
    'K2HPO4',
    'FeSO4',
    'NH4Cl',
    'MgCl2',
    'NaCl',
    '(NH4)6Mo7O24',
    'CoCl2',
    'CuSO4',
    'MnSO4',
    'ZnSO4'
]

user_params['response'] = 'OD340'


# Here we specify how many instances (designs) we want to create and how many replicates: 


user_params['n_instances_explor'] = 8
user_params['n_instances_exploit'] = 7
user_params['n_replicates'] = 3


# ## Load the data


study_slug_1 = user_params['study_slug_1']
study_slug_2 = user_params['study_slug_2']
study_slug_3 = user_params['study_slug_3']
edd_server = user_params['edd_server']
username = user_params['username']


try:
    session = eddu.login(edd_server=edd_server, user=username)
except:
    print('ERROR! Connection to EDD failed. We will try to load data from disk...')
else:
    print('OK! Connection to EDD successful. We will try to load data from EDD...')


try:
    df_1 = eddu.export_study(session, study_slug_1, edd_server=edd_server)
    df_2 = eddu.export_study(session, study_slug_2, edd_server=edd_server)
    df_3 = eddu.export_study(session, study_slug_3, edd_server=edd_server)
except (NameError, AttributeError, KeyError):
    print(f'ERROR! Not able to export the study.')




df_1.head(2)


df_2.head(2)


df_3.head(2)


# Concatenate the three studies:


df = df_1.append(df_2).append(df_3)


# Drop unnecessary columns:


df = df.loc[:,['Line Name','Line Description','Protocol','Value']]
df.head()


# Pivot the dataframe to include columns for all protocols:


df = df.pivot(index=["Line Name", "Line Description"], columns="Protocol", values="Value")
df.reset_index(inplace=True)
df.head()


# ### Adding media information to the data frame


# Add columns for each component:


components = re.split(': |, ', df['Line Description'][0])[::2]
for comp in components:
    df[comp] = None



# And assign values for each component and line:


for i in range(len(df)):
    values = re.split(': |, ', df['Line Description'][i])[1::2]
    for c, value in enumerate(values):
        df.iloc[i, (4+c)] = float(value)

df.drop(columns='Line Description', inplace=True)
df.tail()


# Define the control lines. In DBTL 1 and 2, controls were wells F5 to F8. In DBTL 3 is stored in the last column (D8, E8, F8).


control_lines = df[df['Line Name'].str.find('WF5_F8') > 0]

control_lines = control_lines.append(df[df['Line Name'].str.find('WD8_F8') > 0])


control_lines


# How many designs improve response over the standard recipe?


control_response = np.max(control_lines['OD340'])
num_improved_response = np.sum(df['OD340'] > control_response)
num_designs = len(df) - len(control_lines)
print(f'{num_improved_response} samples out of {num_designs} improve over the standard in terms of OD340 ({num_improved_response/num_designs*100:.2f}%).')



# ### Convert the data to EDD format


# Pivot the dataframe back to EDD format, including all the components names and protocols:


df_stacked = df.set_index('Line Name').stack().reset_index()
df_stacked.columns = ['Line Name', 'Measurement Type', 'Value']
df_stacked.head()


# # Media designs recommendations


# ## ART generated media


# Define a dictionary that contains the settings that ART will use to find the recommended designs:


art_params = {
    'input_vars': user_params['components'],
    'response_vars': [user_params['response']],
    'bounds_file': user_params['bounds_file'], # file with bounds# input variables, i.e. features
    'seed': 10,                                           # seed for number random generator
    'cross_val': True,
    'recommend': False,
    'output_directory': user_params['output_file_path'],  # directory to store this output
    'verbose': 1
}



# We will first run only model building part and then proceed with optimization using the two modes - exploration and exploitation>


user_params['alpha_explor'] = 1.0
user_params['alpha_exploit'] = None


# With the configuration stored in art_params, we now run ART:


run_art = True


%%time
warnings.filterwarnings("ignore")
if run_art:
    art = RecommendationEngine(df_stacked, **art_params)
else:
    with open(os.path.join(art_params['output_directory'], 'art.pkl'), 'rb') as output:
        art = pickle.load(output)


art.evaluate_models_cv()


# ### Exploitation recommendations


%%time
art.niter = 100000
art.alpha = user_params['alpha_exploit']
draws = art.parallel_tempering_opt()



orig_file_name = f"{art_params['output_directory']}/draws.txt"
new_file_name = f"{art_params['output_directory']}/draws_exploit.txt"
os.rename(orig_file_name, new_file_name)


art.num_recommendations = user_params['n_instances_exploit'] # 7 in this DBTL4
art.rel_rec_distance = 2. # Default is 0.2
# At least one of the features in the recommendation must differ by this relative factor from any point in the training data and any previous recommendations
# Then pick the one with the best objective function value and add to the recommendations. Repeat the distance filtering above. 
# Stops when 7 recommendations are found.

# If it cannot find enough recommendations, it will decrease the distance by 0.80x and try again.
art.recommend(draws)



df_rec_exploit = art.recommendations.copy()


# Add standard deviation for the predictions:


predicted_mean, predicted_std = art.post_pred_stats(
            df_rec_exploit.values[:, :-1]
        ) # posterior predictive statistics: mean and std

df_rec_exploit['OD340_std'] = predicted_std
df_rec_exploit['Label'] = 'exploitation' # will be used for plotting in the parity plot in notebook F


plot.draws_pc(art, draws, scale=100, plot_draws=True, plot_kde=True)
plot.recommendations_pc(art, scale=100)



plot.model_pairwise(art)


# ### Exploration recommendations


%%time
art.niter = 100000
art.alpha = user_params['alpha_explor']
draws = art.parallel_tempering_opt()



orig_file_name = f"{art_params['output_directory']}/draws.txt"
new_file_name = f"{art_params['output_directory']}/draws_explor.txt"
os.rename(orig_file_name, new_file_name)


art.num_recommendations = user_params['n_instances_explor']
art.rel_rec_distance = 3.
art.recommend(draws)


plot.draws_pc(art, draws, scale=100, plot_draws=True, plot_kde=True)
plot.recommendations_pc(art, scale=100)



df_rec_explor = art.recommendations.copy()



predicted_mean, predicted_std = art.post_pred_stats(
            df_rec_explor.values[:, :-1]
        )

df_rec_explor['OD340_std'] = predicted_std
df_rec_explor['Label'] = 'exploration'


df_rec = df_rec_exploit.append(df_rec_explor)


df_rec = df_rec.rename(columns={"OD340": "OD340_pred"})


# ### Generate the control media 


# Read the standard recipe:


df_stand = pd.read_csv(user_params['standard_media_file']).set_index("Component")


# Control media will be uniformly drawn from the interval 90% to 110% around the standard recipe.


ub = 1.1
lb = 0.9
df_control = pd.DataFrame(columns=user_params['components'])

for component in user_params['components']:
    stand_conc = df_stand.loc[component]['Concentration[mM]']
    df_control.at['Control', component] = stand_conc*np.random.uniform(lb, ub)

df_control


# Check that these controls are not equivalent to the controls tested in previous cycles:


for component in user_params['components']:
     assert(all(df_control.at['Control', component] != control_lines[component]))


# Add ART's prediction for this design


control_predicted_mean, control_predicted_std = art.post_pred_stats(
            df_control.values
        )
df_control['OD340_pred'] = control_predicted_mean
df_control['OD340_std'] = control_predicted_std
df_control['Label'] = 'standard'


df_rec = df_rec.append(df_control).reset_index(drop=True)
df_rec


# ### Check the distribution of designs


# Define dataframe with train data and predictions:


df_train = df[user_params['components']]
df_train['OD340'] = df['OD340']


# Add predictions, cv-predictions and standard deviation of the OD340 predictions:


train_predicted_mean, train_predicted_std = art.post_pred_stats(
            df_train.values[:, :-1]
        )


df_train['OD340_pred'] = train_predicted_mean
df_train['OD340_std'] = train_predicted_std
df_train['OD340_cv_pred'] = art.model_df_cv[0]["Predictions"]["Ensemble Model"]
df_train['OD340_cv_std'] = art.model_df_cv[0]["Predictions StDev"]["Ensemble Model"]


# Add label for control lines:


df_train['Label'] = 'train'
df_train.loc[control_lines.index, 'Label'] = 'standard'


df_train


# Save train data set for this cycle:


file = f"{user_params['output_file_path']}/train_pred.csv"
df_train.to_csv(file)


def designs_pairwise(art, df_rec, user_params, df_train=None):

    dim = art.num_input_var

    plt.style.use('seaborn-whitegrid')

    fig = plt.figure(figsize=(35, 35))
    fig.patch.set_facecolor("white")

    X = df_rec[user_params['components']].values
    X_train = df_train[user_params['components']].values
    standard = df_train[df_train['Label']=='standard'].drop(columns='Label').values

    for var1 in range(dim):
        for var2 in range(var1 + 1, dim):

            ax = fig.add_subplot(dim, dim, (var2 * dim + var1 + 1))
            ax.scatter(
                X_train[:, var1],
                X_train[:, var2],
                c="r",
                marker="+",
                s=150*df_train['OD340'],
                lw=1,
                label="Train data",
            )
            
            ax.scatter(
                standard[:, var1],
                standard[:, var2],
                c="k",
                marker="+",
                s=150*standard[:, -1].astype(float),
                lw=1,
                label="Standard",
            )
            
            ax.scatter(
                X[:, var1],
                X[:, var2],
                c="g",
                marker="+",
                s=150*df_rec['OD340_pred'],
                lw=1,
                label="Recommendations",
            )
            
            ax.scatter(
                X[-1, var1],
                X[-1, var2],
                c="k",
                marker="+",
                s=150*df_rec['OD340_pred'].values[-1],
                lw=1,
                label="Standard",
            )
            
            
                        
            if var2 == (dim - 1):
                ax.set_xlabel(art.input_vars[var1])
            if var1 == 0:
                ax.set_ylabel(art.input_vars[var2])
                if var2 == 0:
                    ax.legend(loc="center left", bbox_to_anchor=(1, 0.5), shadow=True)

    fig.savefig(
        f'{art.outDir}/designs_pairwise.png',
        bbox_inches="tight",
        transparent=False, 
        dpi=300
    )


# Red are train data, green are recommendations, black are standards


designs_pairwise(art, df_rec, user_params, df_train)


# ## Saving the generated designs


# Include all replicates: 


df_rec = df_rec.loc[df_rec.index.repeat(user_params['n_replicates'])]



# Rename index to well names:


well_rows = 'ABCDEF'
well_columns = '12345678'


well_names = [f'{row}{column}'  for column in well_columns for row in well_rows]

df_rec['Well'] = well_names
df_rec = df_rec.set_index(['Well'])
df_rec.head()


file = f"{user_params['output_file_path']}/target_concentrations.csv"
df_rec.to_csv(file) # This file will be used for the computation of volume transfers and data analysis





